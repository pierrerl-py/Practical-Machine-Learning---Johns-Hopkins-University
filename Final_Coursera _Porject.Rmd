# Coursera Final Project
#### Title: Practical Machine Learning Project - Quantified Self Movement Data Analysis Report
#### Author: Pierre Roux-Lafargue
 
## Background
##### Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the quantified self movement â€“ a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks. One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it.

##### The goal of this project is to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants to create a machine learning algorythm to measure how well the exercises were performed. The participants were asked to perform barbell lifts correctly and incorrectly in 5 different ways.

##### More information is available from the website here: #####http://web.archive.org/web/20161224072740/http:/groupware.les.inf.puc-rio.br/har (see the section on the Weight Lifting Exercise Dataset).


## Model Discussion
##### After cleaning up the data, three different machine learning models were used to estimate the classe variable on the dataset.

#### Classification Trees

##### For the classification tree algorithm, the goal is to split the data at a node. Each split is based on a combination of all the explanatory variables of the model. The goal at each split is to increase the homogeneity of each resulting branch. To measure this homogeneity, different measures can be used depending on whether the machine learning algorithm categorial (classification) or numerical (regression). In our case, the split was done based on the Gini index which is the default in the rpart package.Note that by using rpart through the caret package, the size of the tree is automatically pruned. This is to avoid building a decision tree which is too complex and which would overfit the data. Therefore, caret prunes back the tree. This creates a simpler tree with less splits and an enhanced interpretation, at the cost of a little bias 

#### Random Forest
##### In our model, we tuned the random forest algorithm to produce 500 trees. While more trees generally improve the model, each additional tree has more computational costs (time) and after a certain number of trees, the added benefit of including another tree decreases. Ntree = 500 (which is the default in the caret package) is widely used measure in the literature and was therefore used. Regarding the number of selected bootstrap variables at each node, a general rule in the literature is to use the square root of the number of features. We followed this rule and our model therefore bootstrapped 7 features (mtry = 7) from the 53 total features ( sqrt(53) = 7.28 ). Finally we used 5-fold cross validation in order to improve the estimated accuracy of our model. 

#### Boosting
##### For our application we will use the Gradient Boosting Machine (gbm) package. This method uses decision trees as classifier and aggregates them in order to optimize prediction.  In the package the important things to consider are 1) n.trees, 2) interaction.depth and 3) shrinkage. While all these values could be tuned differently, the caret package optimizes them. We therefore left the default values. We also cross validated our model 5 times, like in the random forest model, in order to improve out of sample error predictably of our model. From the caret package the optimized values lead to the construction of gbm model which had a total of 150 trees, each tree had a depth of 3 and the shrinkage = 0.1. These tuning parameters resulted in an out of sample accuracy measure of 98.78% which is better than the classification tree but slightly worse than the random forest model. 

## Conclusion
##### The accuracy measure (true positives and true negatives divided by the number of true positives, true negatives, false positives, and false negatives) of the models were (in increasing order): 1) classification tree algorithm (57.28%) 2) boosting machine algorithm (98.78%) and 3) random forest algorithm (99.78%). As such, the out of sample errors are equal to 42.72%, 1.22% and 0.22% respectively. The best model was therfore the random forest algorithm. Its prediction on the testing set of the classe variable can be found at the end of the document.
 
## Code
 ``` {r}
 # Loading the required packages
library(caret)
library(rpart)
library(corrplot)
library(httr)
library(RCurl)
library(rattle)
```

 ``` {r}
# Getting the data
testing <- read.csv("https://raw.githubusercontent.com/pierrerl-py/Practical-Machine-Learning---Johns-Hopkins-University/master/pml-testing.csv")
training <- read.csv("https://raw.githubusercontent.com/pierrerl-py/Practical-Machine-Learning---Johns-Hopkins-University/master/pml-training.csv")
```

### Cleaning up the data.

 ``` {r}
# Removing predictors which are not useful (either have near 0 variance or are N/A or have little predictive power).
NZV <- nearZeroVar(training, saveMetrics = TRUE)
training <- training[, !NZV$nzv]
testing <- testing[, !NZV$nzv]
rm(NZV)
  
keep_not_NA <- (colSums(is.na(training)) == 0)
training <- training[, keep_not_NA]
testing <- testing[, keep_not_NA]
rm(keep_not_NA)

regex <- grepl("^X|timestamp|user_name", names(training))
training <- training[, !regex]
testing <- testing[, !regex]
rm(regex)
dim(training)
```

### Partitionaing the training data to have a validation set. 

 ``` {r}
set.seed(4321) # to replicate.

inTrain <- createDataPartition(training$classe, 
                               p = 0.70, 
                               list = FALSE)

validation <- training[-inTrain, ]
training <- training[inTrain, ]
rm(inTrain) 
```

### Modeling.
#### Classification Tree

 ``` {r}
set.seed(4321)
mod_ct <- train(classe ~ ., data=training, method = "rpart")

# Ploting the model.

fancyRpartPlot(mod_ct$finalModel)

# Testing the model.
pred_ct <- predict(mod_ct, validation)
CM_ct <- confusionMatrix(pred_ct, validation$classe) #statistics to see accuracy of model
CM_ct

varImp(mod_ct)

# Plot the model
plot(CM_ct$table, col = CM_ct$byClass, 
     main = paste("Decision Tree - Accuracy =", round(CM_ct$overall['Accuracy'], 4)))
```

#### Random Forest

 ``` {r}
Control_rf = trainControl(method = "cv", 5, verboseIter = FALSE)

set.seed(4321)
mtry <- sqrt(length(names(training)))
tunegrid <- expand.grid(.mtry=mtry)
mod_rf <- train(classe~., 
                    data=training, 
                    method='rf', 
                    metric='Accuracy', 
                    tuneGrid=tunegrid, 
                    trControl=Control_rf)

mod_rf
mod_rf$finalModel$ntree

  # Testing the model.
pred_rf <- predict(mod_rf, validation)
CM_rf <- confusionMatrix(pred_rf, validation$classe) #statistics to see accuracy of model
CM_rf

  # Ploting the model

plot(CM_rf$table, col = CM_rf$byClass, 
     main = paste("Decision Tree - Accuracy =", round(CM_rf$overall['Accuracy'], 4)))
```

#### Boosting 
 
 ``` {r}
Control_gbm <- trainControl(method = "cv", 5)

set.seed(4321)
mod_gbm <- train(classe~., data = training, trControl = Control_gbm, method = "gbm", verbose = FALSE)
print(mod_gbm)

# Testing the model.
pred_gbm <- predict(mod_gbm, validation)
CM_gbm <- confusionMatrix(pred_gbm, validation$classe) #statistics to see accuracy of model

trellis.par.set(caretTheme())
plot(mod_gbm)  

# Plot the model
plot(CM_gbm$table, col = CM_gbm$byClass, 
     main = paste("Decision Tree - Accuracy =", round(CM_gbm$overall['Accuracy'], 4)))
```

### Comparing the three alogrithms.
 ``` {r}
CM_ct$overall['Accuracy']
CM_rf$overall['Accuracy']
CM_gbm$overall['Accuracy']
```

### Comparing the three alogrithms out of sample error.
 ``` {r}
# Classification Tree Out of Sample Error
1 - CM_ct$overall['Accuracy']

# Random Out of Sample Error
1 - CM_rf$overall['Accuracy']

# Boosting Out of Sample Error
1 - CM_gbm$overall['Accuracy']
```

### Testing best model
 ``` {r}
results <- predict(mod_rf, testing)
results
```










